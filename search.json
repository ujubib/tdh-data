[
  {
    "objectID": "slides.html#todays-agenda",
    "href": "slides.html#todays-agenda",
    "title": "Personal and Research Data Management",
    "section": "Today’s Agenda",
    "text": "Today’s Agenda\n\nIntroduction: Research data & FAIR principles (10 min)\nArchifiltre: Explore your data organization (20 min)\nBreak (5 min)\nStoring & Backing up: Best practices (20 min)\nOpenRefine: Hands-on data cleaning (60 min)\nQ&A & Wrap-up (5 min)",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#learning-objectives",
    "href": "slides.html#learning-objectives",
    "title": "Personal and Research Data Management",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this workshop, you will be able to:\n\nUnderstand key methods for organizing research data\nEffectively sort and structure your datasets\nUse powerful tools for data cleaning and quality control",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#what-are-research-data",
    "href": "slides.html#what-are-research-data",
    "title": "Personal and Research Data Management",
    "section": "What are Research Data?",
    "text": "What are Research Data?\n\nFactual records (numerical scores, textual records, images and sound) used as primary sources for scientific research, and that are commonly accepted in the scientific community as necessary to validate research findings.\n\nSource : OECD, OECD Principles and Guidelines for Access to Research Data from Public Funding, Paris, 2007\n\nOECD definition (2007)",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#the-research-data-lifecycle",
    "href": "slides.html#the-research-data-lifecycle",
    "title": "Personal and Research Data Management",
    "section": "The Research Data Lifecycle",
    "text": "The Research Data Lifecycle\n\nSource: Callisto",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#fair-principles",
    "href": "slides.html#fair-principles",
    "title": "Personal and Research Data Management",
    "section": "FAIR Principles",
    "text": "FAIR Principles\n\nImage credit: ARDC licensed under a Creative Commons Attribution 4.0 International License",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#fair-principles-explained",
    "href": "slides.html#fair-principles-explained",
    "title": "Personal and Research Data Management",
    "section": "FAIR Principles Explained",
    "text": "FAIR Principles Explained\n\n\nFindable\n\nPersistent identifiers\nRich metadata\n\nAccessible\n\nOpen protocols\nClear access conditions\n\n\nInteroperable\n\nStandard formats\nVocabularies & ontologies\n\nReusable\n\nClear licensing\nProvenance information",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#why-this-matters",
    "href": "slides.html#why-this-matters",
    "title": "Personal and Research Data Management",
    "section": "Why This Matters",
    "text": "Why This Matters\n\nReproducibility of research\nCollaboration with colleagues\nCompliance with funder requirements\nLong-term preservation of your work\nIncreased visibility and citations",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#fair-open",
    "href": "slides.html#fair-open",
    "title": "Personal and Research Data Management",
    "section": "FAIR / OPEN",
    "text": "FAIR / OPEN\n\nImage source : Consortium of European Social Science Data Archives (CESSDA)",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#fair-care",
    "href": "slides.html#fair-care",
    "title": "Personal and Research Data Management",
    "section": "FAIR / CARE",
    "text": "FAIR / CARE\n\nImage source : Consortium of European Social Science Data Archives (CESSDA)\nThe CARE principles for Indigenous Data Governance (Collective benefit, Authority to control, Responsibility, and Ethics) specifically concern Indigenous data and govern the control and (re)use of such data under the principles of open science (Carroll et al. 2020). For instance, the Collective Benefit principle states that data ecosystems should be designed and function in ways that enable Indigenous Peoples to derive benefit from the data, e.g. for improved government and citizen engagement. The CARE principles are complementary to the FAIR principles, under the motto ‘be FAIR and CARE’ (Carroll et al. 2020).",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#file-naming-conventions",
    "href": "slides.html#file-naming-conventions",
    "title": "Personal and Research Data Management",
    "section": "File Naming Conventions",
    "text": "File Naming Conventions\nGood examples:\n2026-02-09_interview_participant-01_v1.docx\nexperiment-data_2026-01_temperature.csv\nthesis_chapter-03_draft-02.pdf\nBad examples:\nFinal.docx\nFinal_FINAL.docx\ndata.csv\nthesis new version (1).pdf",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#file-naming-rules",
    "href": "slides.html#file-naming-rules",
    "title": "Personal and Research Data Management",
    "section": "File Naming Rules",
    "text": "File Naming Rules\n\nUse dates: YYYY-MM-DD format (ISO 8601)\nNo spaces: use hyphens - or underscores _\nBe descriptive: but concise\nVersion numbers: v01, v02 or draft-01, draft-02\nAvoid special characters: é, à, /, , ?, *, etc.\nUse lowercase: for consistency",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#directory-structure",
    "href": "slides.html#directory-structure",
    "title": "Personal and Research Data Management",
    "section": "Directory Structure",
    "text": "Directory Structure\nmy-research-project/\n├── 01-raw-data/\n├── 02-processed-data/\n├── 03-analysis/\n├── 04-results/\n├── 05-manuscripts/\n├── documentation/\n│   ├── codebook.md\n│   └── README.md\n└── archive/",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#what-is-archifiltre",
    "href": "slides.html#what-is-archifiltre",
    "title": "Personal and Research Data Management",
    "section": "What is Archifiltre?",
    "text": "What is Archifiltre?\n\nArchifiltre is a free tool to:\n\nVisualize your file tree structure\nIdentify duplicates and unused files\nClean up your directories\nDocument your data organization\nPrepare data for archiving or sharing",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#archifiltre-features",
    "href": "slides.html#archifiltre-features",
    "title": "Personal and Research Data Management",
    "section": "Archifiltre Features",
    "text": "Archifiltre Features\n\n\nVisual Analysis\n\nInteractive treemap\nFile size visualization\nDate analysis\nType distribution\n\n\nPractical Tools\n\nDuplicate detection\nBatch operations\nTagging system\nExport reports",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#live-demo-archifiltre",
    "href": "slides.html#live-demo-archifiltre",
    "title": "Personal and Research Data Management",
    "section": "Live Demo: Archifiltre",
    "text": "Live Demo: Archifiltre\n\n\n\n\n\n\nFollow along!\n\n\n\nOpen Archifiltre on your computer\nSelect a research folder to analyze\nExplore the visualization\n\n\n\n\nWhat to look for:\n\nLarge files taking up space\nOld files not modified recently\nDuplicate files\nPoorly organized subdirectories",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#hands-on-exercise-10-min",
    "href": "slides.html#hands-on-exercise-10-min",
    "title": "Personal and Research Data Management",
    "section": "Hands-On Exercise (10 min)",
    "text": "Hands-On Exercise (10 min)\n\nChoose one of your research folders\nLoad it into Archifiltre\nIdentify:\n\nDuplicates\nFiles you could delete or archive\nFolders that need reorganization\n\n\n\n\n\n\n\n\nTip\n\n\nDon’t delete anything yet! Just explore and take notes.",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#the-3-2-1-backup-rule",
    "href": "slides.html#the-3-2-1-backup-rule",
    "title": "Personal and Research Data Management",
    "section": "The 3-2-1 Backup Rule",
    "text": "The 3-2-1 Backup Rule\n\n\n3 copies of your data\nOn 2 different media types\nWith 1 copy off-site",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#storage-solutions-at-uppa",
    "href": "slides.html#storage-solutions-at-uppa",
    "title": "Personal and Research Data Management",
    "section": "Storage Solutions at UPPA",
    "text": "Storage Solutions at UPPA\nNuage (Nextcloud)\n\nhttps://nuage.univ-pau.fr\nUniversity-hosted cloud storage\nSync clients for desktop & mobile\nCollaborative features\n\n\n\n\n\n\n\nNote\n\n\nIdeal for active research data and collaboration",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#national-solutions-huma-num",
    "href": "slides.html#national-solutions-huma-num",
    "title": "Personal and Research Data Management",
    "section": "National Solutions: Huma-Num",
    "text": "National Solutions: Huma-Num\nShareDocs\n\nFor humanities & social sciences researchers\nFileRun-based platform\nCompatible with Nextcloud clients\nDocumentation: https://documentation.huma-num.fr",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#file-formats-for-preservation",
    "href": "slides.html#file-formats-for-preservation",
    "title": "Personal and Research Data Management",
    "section": "File Formats for Preservation",
    "text": "File Formats for Preservation\n\n\nPreferred formats:\n\nText: .txt, .pdf/A, .xml\nImages: .tiff, .png, .jpg\nData: .csv, .json, .xml\nVideo: .mp4, .mkv\n\n\nAvoid if possible:\n\nProprietary formats\nOutdated formats\nFormats requiring specific software\nEncrypted formats (for archiving)",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#resources",
    "href": "slides.html#resources",
    "title": "Personal and Research Data Management",
    "section": "Resources",
    "text": "Resources\n\n\n\n\n\n\nDORANum - French Research Data Portal\n\n\n\nFile naming guide: https://doranum.fr\nStorage & archiving best practices\nFormat recommendations\nVideo tutorials (in French)\n\n\n\n\n\n\n\n\n\n\nStanford - University Libraries\n\n\n\nData best practices and case studies\nName files\nFormat files\nSensitive data",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#what-is-openrefine",
    "href": "slides.html#what-is-openrefine",
    "title": "Personal and Research Data Management",
    "section": "What is OpenRefine?",
    "text": "What is OpenRefine?\n\nOpenRefine is a powerful tool for:\n\nCleaning messy data\nTransforming data formats\nReconciling data with external sources\nExtending data with additional information",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#why-use-openrefine",
    "href": "slides.html#why-use-openrefine",
    "title": "Personal and Research Data Management",
    "section": "Why Use OpenRefine?",
    "text": "Why Use OpenRefine?\n\nWorks with large datasets (millions of rows)\nNon-destructive: keeps history of all operations\nReproducible: export and reuse cleaning scripts\nNo programming required (but supports expressions)\nFree and open-source",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#common-data-quality-issues",
    "href": "slides.html#common-data-quality-issues",
    "title": "Personal and Research Data Management",
    "section": "Common Data Quality Issues",
    "text": "Common Data Quality Issues\n\n\nStructure issues:\n\nInconsistent formatting\nExtra whitespace\nMixed case (UPPER/lower)\nWrong data types\n\n\nContent issues:\n\nDuplicates\nMissing values\nSpelling variations\nInconsistent dates",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#live-demo-openrefine-interface",
    "href": "slides.html#live-demo-openrefine-interface",
    "title": "Personal and Research Data Management",
    "section": "Live Demo: OpenRefine Interface",
    "text": "Live Demo: OpenRefine Interface\nKey components:\n\nProject creation\nData preview\nFacets & filters\nCell transformations\nHistory & undo",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#hands-on-exercise-dirty-data",
    "href": "slides.html#hands-on-exercise-dirty-data",
    "title": "Personal and Research Data Management",
    "section": "Hands-On Exercise: Dirty Data",
    "text": "Hands-On Exercise: Dirty Data\nWe’ll clean a messy dataset together\n\n\nDownload dirty_data.csv\n\n\nIssues in the data:\n\nName: inconsistent capitalization, extra spaces\nEmail: duplicates\nDate: multiple formats, missing values\nCountry: inconsistent capitalization\nScore: “N/A” values mixed with numbers",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#step-1-load-the-data",
    "href": "slides.html#step-1-load-the-data",
    "title": "Personal and Research Data Management",
    "section": "Step 1: Load the Data",
    "text": "Step 1: Load the Data\n\nClick “Create Project”\nChoose “This Computer”\nSelect dirty_data.csv\nClick “Next”\nVerify preview and click “Create Project”",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#step-2-clean-the-name-column",
    "href": "slides.html#step-2-clean-the-name-column",
    "title": "Personal and Research Data Management",
    "section": "Step 2: Clean the “Name” Column",
    "text": "Step 2: Clean the “Name” Column\nRemove extra whitespace:\n\nClick dropdown on “Name” column\nSelect: Edit cells → Common transforms → Trim leading and trailing whitespace\n\nStandardize capitalization:\n\nSame menu: Common transforms → To titlecase",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#step-3-find-duplicates-in-email",
    "href": "slides.html#step-3-find-duplicates-in-email",
    "title": "Personal and Research Data Management",
    "section": "Step 3: Find Duplicates in “Email”",
    "text": "Step 3: Find Duplicates in “Email”\n\nClick dropdown on “Email” column\nSelect: Facet → Text facet\nLook at the facet panel (left side)\nClick on “Sort by: count”\nDuplicates appear at the top!\n\n\n\n\n\n\n\nTip\n\n\nIn a real project, you’d decide how to handle duplicates (merge, keep first, etc.)",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#step-4-standardize-dates",
    "href": "slides.html#step-4-standardize-dates",
    "title": "Personal and Research Data Management",
    "section": "Step 4: Standardize Dates",
    "text": "Step 4: Standardize Dates\nTransform date format:\n\nClick dropdown on “Date d’inscription” column\nSelect: Edit cells → Transform...\nEnter expression:\n\nvalue.replace(\"/\", \"-\").replace(\" \", \"-\")\n\nPreview and click OK",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#step-5-clean-country-column",
    "href": "slides.html#step-5-clean-country-column",
    "title": "Personal and Research Data Management",
    "section": "Step 5: Clean “Country” Column",
    "text": "Step 5: Clean “Country” Column\nCombine operations:\n\nClick dropdown on “Country” column\nTrim whitespace (as before)\nApply titlecase (as before)\n\nCheck results with facet:\n\nFacet → Text facet\nVerify all countries are now standardized",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#step-6-fix-score-column",
    "href": "slides.html#step-6-fix-score-column",
    "title": "Personal and Research Data Management",
    "section": "Step 6: Fix “Score” Column",
    "text": "Step 6: Fix “Score” Column\nReplace “N/A” with null:\n\nClick dropdown on “Score” column\nSelect: Edit cells → Transform...\nEnter expression:\n\nif(value == \"N/A\", null, value)\n\nClick OK",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#step-7-export-clean-data",
    "href": "slides.html#step-7-export-clean-data",
    "title": "Personal and Research Data Management",
    "section": "Step 7: Export Clean Data",
    "text": "Step 7: Export Clean Data\n\nClick “Export” (top right)\nChoose format: “Comma-separated value”\nSave as clean_data.csv\n\nDon’t forget:\n\nYou can export your cleaning operations (JSON) to reuse them!\nClick Undo / Redo tab → Extract...",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#openrefine-best-practices",
    "href": "slides.html#openrefine-best-practices",
    "title": "Personal and Research Data Management",
    "section": "OpenRefine Best Practices",
    "text": "OpenRefine Best Practices\n\nAlways work on a copy of your data\nUse facets to explore data before transforming\nCheck the preview before applying transformations\nTake notes of your cleaning steps\nExport operations for reproducibility\nValidate results after each major transformation",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#going-further-with-openrefine",
    "href": "slides.html#going-further-with-openrefine",
    "title": "Personal and Research Data Management",
    "section": "Going Further with OpenRefine",
    "text": "Going Further with OpenRefine\nAdvanced features:\n\nReconciliation: match your data with Wikidata, VIAF, etc.\nClustering: find similar values automatically\nGREL expressions: powerful transformation language\nAPI calls: enrich data from external sources\n\nLearn more:\n\nProgramming Historian tutorials\nOpenRefine documentation: https://openrefine.org/docs",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#what-we-covered-today",
    "href": "slides.html#what-we-covered-today",
    "title": "Personal and Research Data Management",
    "section": "What We Covered Today",
    "text": "What We Covered Today\n\nResearch data & FAIR principles\nFile organization best practices\nArchifiltre for visualizing & organizing\nStorage & backup strategies\nOpenRefine for data cleaning",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#key-takeaways",
    "href": "slides.html#key-takeaways",
    "title": "Personal and Research Data Management",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\n\n\n\n\nRemember\n\n\n\nOrganize early: don’t wait until you have chaos\nName carefully: your future self will thank you\nBackup regularly: 3-2-1 rule\nClean systematically: use tools like OpenRefine\nDocument everything: README files are your friends",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#additional-tools-to-explore",
    "href": "slides.html#additional-tools-to-explore",
    "title": "Personal and Research Data Management",
    "section": "Additional Tools to Explore",
    "text": "Additional Tools to Explore\nTropy - for organizing research images\n\nhttps://tropy.org\nPerfect for archival photos, scanned documents\nPowerfull built-in metadata management\n\n\nMention this as bonus for those working with visual materials",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#resources-documentation",
    "href": "slides.html#resources-documentation",
    "title": "Personal and Research Data Management",
    "section": "Resources & Documentation",
    "text": "Resources & Documentation\nAll materials available:\n\nGithub repository: https://github.com/ujubib/tdh-data\nCompanion Website with additional resources: https://ujubib.github.io/tdh-data\n\nContact:\n\njulien.rabaud@univ-pau.fr",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "slides.html#questions",
    "href": "slides.html#questions",
    "title": "Personal and Research Data Management",
    "section": "Questions?",
    "text": "Questions?\nThank you for your participation\n\n\n\n\n\n\nFeedback\n\n\nPlease share your thoughts on today’s workshop.",
    "crumbs": [
      "Workshop",
      "Slides"
    ]
  },
  {
    "objectID": "resources/file-management.html",
    "href": "resources/file-management.html",
    "title": "File Naming & Backup for Digital Humanities",
    "section": "",
    "text": "Systematic file management is essential for Digital Humanities research, especially for projects involving multiple sources, collaborators, or long-term data collection.\n\n\n\n\n\n\nImportantFor Doctoral Researchers\n\n\n\nWith projects spanning 3-5 years, involving multiple institutions, and generating diverse data types, consistent file naming and backup strategies prevent data loss and enable collaboration.",
    "crumbs": [
      "Additional Resources",
      "File Naming & Backup"
    ]
  },
  {
    "objectID": "resources/file-management.html#introduction",
    "href": "resources/file-management.html#introduction",
    "title": "File Naming & Backup for Digital Humanities",
    "section": "",
    "text": "Systematic file management is essential for Digital Humanities research, especially for projects involving multiple sources, collaborators, or long-term data collection.\n\n\n\n\n\n\nImportantFor Doctoral Researchers\n\n\n\nWith projects spanning 3-5 years, involving multiple institutions, and generating diverse data types, consistent file naming and backup strategies prevent data loss and enable collaboration.",
    "crumbs": [
      "Additional Resources",
      "File Naming & Backup"
    ]
  },
  {
    "objectID": "resources/file-management.html#file-naming-conventions",
    "href": "resources/file-management.html#file-naming-conventions",
    "title": "File Naming & Backup for Digital Humanities",
    "section": "File Naming Conventions",
    "text": "File Naming Conventions\n\nCore Principles\n\n\n\n\n\n\nTipQuick Reference: File Naming Rules\n\n\n\n\nUse dates: YYYY-MM-DD format (ISO 8601) - universal across countries\nNo spaces: Use hyphens - or underscores _\nBe descriptive: Include project, source, version\nVersion numbers: v01, v02, v03 (not “final”)\nNo special characters: Avoid é, à, ç, ñ, /, , ?, *\nLanguage-neutral: Use English or transliterated names for international collaboration\n\nExample for archival photos:\n2025-06-15_paris-natlib_ms-fr-12345_fol-23r_v01.jpg\n(Date_Archive_Manuscript_Folio_Version)",
    "crumbs": [
      "Additional Resources",
      "File Naming & Backup"
    ]
  },
  {
    "objectID": "resources/file-management.html#comprehensive-guides---international",
    "href": "resources/file-management.html#comprehensive-guides---international",
    "title": "File Naming & Backup for Digital Humanities",
    "section": "Comprehensive Guides - International",
    "text": "Comprehensive Guides - International\n\n1. DoRANum Resources (French Platform - English Available)\nDoRANum (Données de la Recherche: Apprentissage Numérique) - France’s premier research data management platform.\n\n\n\n\n\n\nNoteDoRANum in English\n\n\n\nAs of January 2025, DoRANum offers:\n\n9 thematic fact sheets in English (Stor age, File naming, Metadata, etc.)\n9 “Minute” videos with English subtitles (short, practical guides)\nDiscipline-specific learning paths on Callisto platform\n\n\n\nKey English resources:\n\nGeneral RDM: Research Data Management tag\nFile naming: Check the fact sheets section (under “Storage and Archiving”)\nVideos: DoRANum Minutes - now with English subtitles\n\nWhy DoRANum?\n\nDeveloped for French research community (includes CNRS, ANR requirements)\nHumanities-friendly (not just STEM-focused)\nFAIR principles integrated\nFree and openly licensed\n\n\n\n\n2. DARIAH-EU - Digital Research Infrastructure for Arts & Humanities\nThe European reference for DH data management\n\nLink: https://www.dariah.eu/\nTraining materials: https://campus.dariah.eu/\nFocus: Arts, humanities, cultural heritage\n\nEssential resources:\n\nDARIAH Research Data Management Guide\nPARTHENOS Training Materials - Cultural heritage specific\nDigital Humanities Course Registry\n\nBest practices documents:\n\nFile organization for multilingual projects\nManaging transcriptions and editions\nMetadata for cultural objects\n\n\n\n\n3. Stanford Libraries - File Naming Best Practices\nClear, practical, with examples\n\nLink: https://library.stanford.edu/research/data-management-services/data-best-practices/best-practices-file-naming\nCovers: Basic rules, versioning, sequential numbering\nExamples: By discipline including humanities\n\nKey takeaways:\n\n3 elements: date, description, version\nAvoid names &gt;32 characters when possible\nDocument naming conventions in README\n\n\n\n\n4. UK Data Service - Organizing Data\nComprehensive, humanities-inclusive\n\nLink: https://ukdataservice.ac.uk/learning-hub/research-data-management/organise-data/file-naming/\nStrength: Examples from social sciences and humanities\nIncludes: Large project workflows, team collaboration\n\n\n\n\n5. Edinburgh Research Data Service\nDetailed with discipline-specific examples\n\nLink: https://www.ed.ac.uk/information-services/research-support/research-data-service/after/organising-data/file-naming\nPDF Guide: File Naming Conventions\nHighlights: Templates you can adapt, common pitfalls",
    "crumbs": [
      "Additional Resources",
      "File Naming & Backup"
    ]
  },
  {
    "objectID": "resources/file-management.html#naming-conventions-for-specific-dh-data-types",
    "href": "resources/file-management.html#naming-conventions-for-specific-dh-data-types",
    "title": "File Naming & Backup for Digital Humanities",
    "section": "Naming Conventions for Specific DH Data Types",
    "text": "Naming Conventions for Specific DH Data Types\n\nTranscriptions\n[date]_[source-archive]_[document-id]_transcription_[version].txt\n\nExample:\n2025-06-20_bnf_ms-lat-8501_fol-12v_transcription_v03.txt\n\n\nDigitized Images\n[date]_[archive]_[collection]_[item-id]_[folio/page]_[version].jpg\n\nExample:\n2025-03-15_vat-lib_pal-lat_1071_fol-45r_v01.tif\n\n\nInterview Recordings\n[date]_interview_[participant-id]_[location]_[recorder].mp3\n\nExample:\n2025-09-10_interview_p05_rome_audio-01.mp3\n\n\n3D Models\n[date]_[site]_[object-type]_[scan-method]_[processing-stage].obj\n\nExample:\n2025-07-22_pompeii_fresco_photogrammetry_raw.obj\n2025-07-22_pompeii_fresco_photogrammetry_cleaned.obj\n\n\nMetadata Files\n[date]_[project]_[data-type]_metadata_[version].csv\n\nExample:\n2025-11-05_choral-manuscripts_images_metadata_v02.csv",
    "crumbs": [
      "Additional Resources",
      "File Naming & Backup"
    ]
  },
  {
    "objectID": "resources/file-management.html#directory-structure-for-dh-projects",
    "href": "resources/file-management.html#directory-structure-for-dh-projects",
    "title": "File Naming & Backup for Digital Humanities",
    "section": "Directory Structure for DH Projects",
    "text": "Directory Structure for DH Projects\n\nRecommended Structure\nchoral-phd-project/\n├── 00-admin/\n│   ├── dmp/                     # Data Management Plans\n│   ├── ethics/                  # Ethics documentation\n│   └── funding/                 # Grant documents\n├── 01-primary-sources/\n│   ├── archives/\n│   │   ├── raw-images/         # Original photos from archives\n│   │   ├── processed/          # Cropped, enhanced versions\n│   │   └── metadata/           # Spreadsheets with source info\n│   ├── transcriptions/\n│   │   ├── draft/\n│   │   └── final/\n│   └── fieldnotes/\n├── 02-secondary-sources/\n│   ├── literature/\n│   └── bibliography/           # Zotero exports, etc.\n├── 03-datasets/\n│   ├── raw/                    # Never modify!\n│   ├── cleaned/                # OpenRefine outputs\n│   └── analysis-ready/\n├── 04-analysis/\n│   ├── scripts/                # R, Python, etc.\n│   ├── outputs/\n│   └── visualizations/\n├── 05-publications/\n│   ├── article-01/\n│   ├── dissertation/\n│   └── conference-posters/\n├── 06-public-engagement/\n│   ├── exhibition-materials/\n│   └── teaching-resources/\n├── documentation/\n│   ├── README.md              # Project overview\n│   ├── codebook.md            # Variable definitions\n│   ├── changelog.md           # Project history\n│   └── file-naming-guide.md   # Your conventions\n└── archive/                    # Deprecated versions",
    "crumbs": [
      "Additional Resources",
      "File Naming & Backup"
    ]
  },
  {
    "objectID": "resources/file-management.html#backup-strategies-the-3-2-1-rule",
    "href": "resources/file-management.html#backup-strategies-the-3-2-1-rule",
    "title": "File Naming & Backup for Digital Humanities",
    "section": "Backup Strategies: The 3-2-1 Rule",
    "text": "Backup Strategies: The 3-2-1 Rule\n\nUnderstanding 3-2-1 for Research Projects\n\n\n\n\n\n\nTipThe 3-2-1 Backup Rule\n\n\n\n\n3 copies of your data\n\nOriginal working copy\nBackup at institution\nBackup at home OR cloud\n\n2 different media types\n\nLocal hard drive (laptop/desktop)\nExternal hard drive OR university server\nCloud storage (Nextcloud, Huma-Num)\n\n1 copy off-site\n\nGeographically separate\nProtected from local disasters (fire, theft)\nAccessible when traveling or working remotely\n\n\n\n\n\n\nPractical Implementation\nChallenge: Managing data across multiple work locations\nSolution:\n\nPrimary working storage: University Nextcloud/cloud (syncs everywhere)\nLocal backup: External hard drive at office or home\nOff-site backup: Institutional server or Huma-Num\n\nBest practices:\n\nKeep active projects on cloud storage for easy access\nCarry external drive for large files (videos, high-res images)\nTest restoration periodically to verify backups work",
    "crumbs": [
      "Additional Resources",
      "File Naming & Backup"
    ]
  },
  {
    "objectID": "resources/file-management.html#cloud-storage-solutions---european-fair-compliant",
    "href": "resources/file-management.html#cloud-storage-solutions---european-fair-compliant",
    "title": "File Naming & Backup for Digital Humanities",
    "section": "Cloud Storage Solutions - European & FAIR-compliant",
    "text": "Cloud Storage Solutions - European & FAIR-compliant\n\n1. Institutional Nextcloud (UPPA Nuage)\nUPPA’s Nextcloud instance\n\nURL: https://nuage.univ-pau.fr\nCapacity: Generous (check with IT)\nFeatures:\n\nAutomatic sync across devices\nFile sharing (internal/external)\nCollaborative editing (OnlyOffice)\nGDPR compliant (data in France)\n\n\nBest for: Active research files, collaboration within UPPA\n\n\n\n2. Huma-Num ShareDocs (For Humanities Research)\nNational infrastructure for humanities & social sciences\n\nURL: https://www.huma-num.fr/services-et-outils\nDocumentation: https://documentation.huma-num.fr\n\nTwo modes:\n\nStorage mode: Like Nextcloud, for active files\nProcessing mode: For data preparation before archiving\n\nBest for:\n\nHumanities-specific workflows\nCollaboration with researchers outside UPPA\nPreparing data for long-term archiving\n\nAlso from Huma-Num:\n\nNakala: Data repository for research data (FAIR-compliant)\nISIDORE: Search engine for SSH research\n\n\n\n\n3. DARIAH-DE Repository\nFor DH projects in the DARIAH network\n\nURL: https://de.dariah.eu/repository\nPurpose: Long-term preservation of DH research data\nAccepts: Texts, corpora, databases, research tools\n\n\n\n\n4. Zenodo - Open Science Repository\nEuropean Commission-funded repository\n\nURL: https://zenodo.org\nFeatures:\n\nDOI for every dataset\nIntegration with GitHub\nVersioning support\nFAIR-compliant\n\n\nPerfect for:\n\nPublishing research data\nSharing datasets linked to publications\nMaking CHORAL research outputs discoverable",
    "crumbs": [
      "Additional Resources",
      "File Naming & Backup"
    ]
  },
  {
    "objectID": "resources/file-management.html#file-formats-for-digital-humanities",
    "href": "resources/file-management.html#file-formats-for-digital-humanities",
    "title": "File Naming & Backup for Digital Humanities",
    "section": "File Formats for Digital Humanities",
    "text": "File Formats for Digital Humanities\n\nPreservation-Friendly Formats by Data Type\n\n\n\n\n\n\nNotePrioritize open, documented formats\n\n\n\nProprietary formats risk obsolescence. Choose formats with:\n\nOpen specifications\nWide tool support\nNo licensing restrictions\nGood for long-term preservation\n\n\n\n\n\n\n\n\n\n\n\n\nData Type\nPreferred Formats\nAvoid\nNotes\n\n\n\n\nPlain text\n.txt, .md\n.pages, .doc\nUTF-8 encoding!\n\n\nStructured text\n.xml (TEI), .json\nProprietary XML\nTEI for scholarly editions\n\n\nTabular data\n.csv, .tsv\n.xlsx, .numbers\nUse semicolons for European locales\n\n\nImages (photos)\n.tiff (archival), .jpg (access)\n.psd, proprietary RAW\nKeep RAW separately\n\n\nImages (diagrams)\n.svg, .png\n.ai, .sketch\nSVG is scalable\n\n\n3D models\n.obj, .ply, .gltf\n.max, .blend\nInclude textures separately\n\n\nAudio\n.wav, .flac\n.mp3 (lossy)\nLossless for primary sources\n\n\nVideo\n.mp4 (H.264), .mkv\n.mov, proprietary codecs\nDocument settings\n\n\nDatabases\n.sql, .csv exports\n.mdb, .accdb\nExport to open formats\n\n\nManuscripts\n.pdf/A, TEI .xml\n.docx, .indd\nPDF/A for archival\n\n\n\n\n\nTEI (Text Encoding Initiative)\nThe DH standard for encoding texts\n\nWebsite: https://tei-c.org/\nWhy use TEI:\n\nScholarly standard for digital editions\nCaptures textual structure and features\nInteroperable across projects\nLong-term preservation\n\n\nLearning resources:\n\nTEI by Example\nWomen Writers Project Guide\nTEI Course at DARIAH",
    "crumbs": [
      "Additional Resources",
      "File Naming & Backup"
    ]
  },
  {
    "objectID": "resources/file-management.html#data-management-plans-dmp",
    "href": "resources/file-management.html#data-management-plans-dmp",
    "title": "File Naming & Backup for Digital Humanities",
    "section": "Data Management Plans (DMP)",
    "text": "Data Management Plans (DMP)\n\nWhy DMPs Matter\nRequired by:\n\nEuropean Commission (Horizon Europe)\nNational funders (ANR, ERC)\nMany doctoral schools\n\nA DMP specifies:\n\nWhat data you’ll collect/create\nHow it will be stored and backed up\nWho can access it\nHow it will be preserved after your PhD\nCosts and responsibilities\n\n\n\nDMP Tools\n\n1. DMP OPIDoR (Recommended for French institutions)\n\nURL: https://dmp.opidor.fr/\nFeatures:\n\nFrench and English interface\nTemplates for ANR, H2020, ERC\nIntegration with institutional systems\nSupport for collaborative projects\n\n\nTutorial: DoRANum DMP Guide (French with some English)\n\n\n2. DMPonline (UK-based, international)\n\nURL: https://dmponline.dcc.ac.uk/\nFeatures:\n\nFunder-specific templates\nGuidance for each question\nInstitutional branding\n\n\n\n\n3. ARGOS (European)\n\nURL: https://argos.openaire.eu/\nFeatures:\n\nPart of OpenAIRE infrastructure\nSupports collaborative editing\nMachine-actionable DMPs\n\n\n\n\n\nDMP Resources\n\nDARIAH DMP Guidelines - Arts & humanities focus\nDCC DMP Checklist - Comprehensive\nScience Europe DMP Evaluation Rubric - What funders look for",
    "crumbs": [
      "Additional Resources",
      "File Naming & Backup"
    ]
  },
  {
    "objectID": "resources/file-management.html#metadata-best-practices-for-cultural-heritage",
    "href": "resources/file-management.html#metadata-best-practices-for-cultural-heritage",
    "title": "File Naming & Backup for Digital Humanities",
    "section": "Metadata Best Practices for Cultural Heritage",
    "text": "Metadata Best Practices for Cultural Heritage\n\nWhy Metadata Matters\nWithout metadata, your data is useless:\n\nWhere was this photo taken?\nWho created this document?\nWhat do these column headers mean?\nCan I reuse this 3D model?\n\n\n\nMetadata Standards for DH\n\n\n\nDomain\nStandard\nLink\n\n\n\n\nCultural Objects\nCIDOC-CRM\nhttp://www.cidoc-crm.org/\n\n\nLibraries\nDublin Core, MARC\nhttps://www.dublincore.org/\n\n\nArchives\nEAD, ISAD(G)\nhttps://www.loc.gov/ead/\n\n\nMuseums\nLIDO, SPECTRUM\nhttp://lido-schema.org/\n\n\nManuscripts\nTEI msDesc\nhttps://tei-c.org/release/doc/tei-p5-doc/en/html/MS.html\n\n\nImages\nIIIF\nhttps://iiif.io/\n\n\nGeographic\nGeoJSON, KML\nhttps://geojson.org/\n\n\n\n\n\nMinimal Metadata for Any File\nAlways include:\n\nCreator/Author: Who made this?\nDate: When (YYYY-MM-DD)?\nDescription: What is it?\nSource: Where did it come from?\nLicense/Rights: Can others use it?\n\nFor images, also include:\n\nArchive/institution\nCollection name\nShelfmark/identifier\nPhysical description",
    "crumbs": [
      "Additional Resources",
      "File Naming & Backup"
    ]
  },
  {
    "objectID": "resources/file-management.html#checklists-templates",
    "href": "resources/file-management.html#checklists-templates",
    "title": "File Naming & Backup for Digital Humanities",
    "section": "Checklists & Templates",
    "text": "Checklists & Templates\n\nFile Organization Checklist\n\n\n\n\n\n\nTipBefore Starting Your PhD\n\n\n\n\nDefine file naming convention (document it!)\nCreate directory structure\nSet up cloud sync (Nextcloud/Huma-Num)\nConfigure external hard drive backup\nWrite initial README file\nDraft Data Management Plan (if required)\n\n\nDuring Your PhD\n\nName files consistently (review quarterly)\nBackup data weekly (automate!)\nUpdate README with changes\nVersion important files clearly\nDocument data collection methods\n\n\n\nBefore Graduation\n\nClean up directory structure\nRemove duplicates\nFinalize all documentation\nDeposit data in repository (Zenodo/Nakala)\nUpdate DMP with final information\n\n\n\n\n\n\nREADME Template for Research Projects\n# Project Title\n\n## Project Information\n- **PI:** Your Name\n- **Institution:** UPPA / ED 481 SSH\n- **Start date:** YYYY-MM-DD\n- **End date:** YYYY-MM-DD\n- **Funding:** [If applicable]\n\n## Project Description\n[2-3 paragraph description of research]\n\n## Directory Structure\n- `/01-primary-sources` - Source materials\n- `/02-secondary-sources` - Literature\n- `/03-datasets` - Structured data\n- [etc.]\n\n## File Naming Convention\n[Explain your system]\n\nExample: `YYYY-MM-DD_source_description_version.ext`\n\n## Data Collection\n- **Archives visited:** [list]\n- **Dates:** [list]\n- **Equipment used:** [camera, scanner, etc.]\n\n## Software & Tools\n- Transcription: [tool]\n- Analysis: [tool]\n- Visualization: [tool]\n\n## Data Formats\n- Images: TIFF (archival), JPEG (access copies)\n- Text: TEI XML for editions, TXT for notes\n- Tabular: CSV with UTF-8 encoding\n\n## Backup Strategy\n- Primary: UPPA Nextcloud\n- Secondary: External HD (weekly)\n- Tertiary: Huma-Num (monthly)\n\n## Rights & Licensing\n- **Data license:** [CC-BY, CC0, etc.]\n- **Code license:** [if applicable]\n- **Restrictions:** [e.g., embargo until publication]\n\n## Contact\n- **Email:** your.email@univ-pau.fr\n- **ORCID:** 0000-0000-0000-0000\n\n## Changelog\n- 2025-09-15: Initial setup\n- 2026-01-10: Added transcriptions folder\n- [etc.]",
    "crumbs": [
      "Additional Resources",
      "File Naming & Backup"
    ]
  },
  {
    "objectID": "resources/file-management.html#during-your-phd",
    "href": "resources/file-management.html#during-your-phd",
    "title": "File Naming & Backup for Digital Humanities",
    "section": "During Your PhD",
    "text": "During Your PhD\n\nName files consistently (review quarterly)\nBackup data weekly (automate!)\nUpdate README with changes\nVersion important files clearly\nDocument data collection methods",
    "crumbs": [
      "Additional Resources",
      "File Naming & Backup"
    ]
  },
  {
    "objectID": "resources/file-management.html#before-graduation",
    "href": "resources/file-management.html#before-graduation",
    "title": "File Naming & Backup for Digital Humanities",
    "section": "Before Graduation",
    "text": "Before Graduation\n\nClean up directory structure\nRemove duplicates\nFinalize all documentation\nDeposit data in repository (Zenodo/Nakala)\nUpdate DMP with final information",
    "crumbs": [
      "Additional Resources",
      "File Naming & Backup"
    ]
  },
  {
    "objectID": "resources/file-management.html#additional-resources---european-international",
    "href": "resources/file-management.html#additional-resources---european-international",
    "title": "File Naming & Backup for Digital Humanities",
    "section": "Additional Resources - European & International",
    "text": "Additional Resources - European & International\n\nTraining Platforms\n\nDARIAH-Campus - DH training courses\nFOSTLH - Heritage science training\nPARTHENOS Training - Research infrastructures\nLibrary Carpentry - Data skills for librarians/researchers\n\n\n\nGuidelines & Standards\n\nFAIR Principles - Findable, Accessible, Interoperable, Reusable\nCARE Principles - For Indigenous data governance\nDublin Core - Metadata standard\nIIIF - International Image Interoperability Framework\n\n\n\nCommunities\n\nADHO - Alliance of Digital Humanities Organizations\nHumanistica - French-speaking DH association\nDHd - German-language DH community\ncenterNet - International DH centers network",
    "crumbs": [
      "Additional Resources",
      "File Naming & Backup"
    ]
  },
  {
    "objectID": "resources/file-management.html#getting-help",
    "href": "resources/file-management.html#getting-help",
    "title": "File Naming & Backup for Digital Humanities",
    "section": "Getting Help",
    "text": "Getting Help\n\n\n\n\n\n\nNoteSupport Resources\n\n\n\nAt UPPA:\n\nJulien Rabaud (SCD & Pôle Numérique): julien.rabaud@univ-pau.fr\nResearch Data Management support\n\nNational (France):\n\nDoRANum FAQ\nHuma-Num support\n\nEuropean:\n\nDARIAH Helpdesk\nCLARIN Service Centres\n\nInternational:\n\nResearch Data Alliance - Global community\nDH Slack channels",
    "crumbs": [
      "Additional Resources",
      "File Naming & Backup"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Personal and Research Data Management Organize, Clean & Enrich",
    "section": "",
    "text": "This website contains all materials for the Personal and Research Data Management: Organize, Clean & Enrich workshop, organized by the UPPA SSH Doctoral School.\n\n\n\n\n\n\nNoteWorkshop Information\n\n\n\n\nDate: February 9, 2026\nDuration: 2 hours\nFormat: Online (via Rendez-Vous)\nLanguage: English\nInstructor: Julien Rabaud\n\n\n\n\n\n\n\n\nArchifiltre - Visualize and analyze your file organization\nOpenRefine - Clean and transform messy datasets\n\n\n\n\n\nResearch data & FAIR principles\nFile naming conventions and directory structures\nStorage and backup strategies (3-2-1 rule)\nHands-on data cleaning exercises\n\n\n\n\n\n\n\n\n\nView Slides\nDownload dirty_data.csv\nGithub Repository\n\n\n\n\n\nFile Organization Tools\nFile Naming & Backup Best Practices\nOpenRefine Tutorials & Videos\n\n\n\n\n\n\nPlease complete the following before the workshop:\n\n\n\n\n\n\nImportantRequired Installations\n\n\n\n\nArchifiltre - Download here\nOpenRefine - Download here\nSample data - Download dirty_data.csv\n\n\n\nTechnical requirements:\n\nStable internet connection\nWorking camera and microphone (for interaction)\nA folder with some of your research files (for Archifiltre exploration)\n\n\n\n\nIf you encounter any installation issues or have questions, please contact:\nJulien Rabaud\njulien.rabaud@univ-pau.fr",
    "crumbs": [
      "Workshop",
      "Home"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Personal and Research Data Management Organize, Clean & Enrich",
    "section": "",
    "text": "This website contains all materials for the Personal and Research Data Management: Organize, Clean & Enrich workshop, organized by the UPPA SSH Doctoral School.\n\n\n\n\n\n\nNoteWorkshop Information\n\n\n\n\nDate: February 9, 2026\nDuration: 2 hours\nFormat: Online (via Rendez-Vous)\nLanguage: English\nInstructor: Julien Rabaud\n\n\n\n\n\n\n\n\nArchifiltre - Visualize and analyze your file organization\nOpenRefine - Clean and transform messy datasets\n\n\n\n\n\nResearch data & FAIR principles\nFile naming conventions and directory structures\nStorage and backup strategies (3-2-1 rule)\nHands-on data cleaning exercises\n\n\n\n\n\n\n\n\n\nView Slides\nDownload dirty_data.csv\nGithub Repository\n\n\n\n\n\nFile Organization Tools\nFile Naming & Backup Best Practices\nOpenRefine Tutorials & Videos\n\n\n\n\n\n\nPlease complete the following before the workshop:\n\n\n\n\n\n\nImportantRequired Installations\n\n\n\n\nArchifiltre - Download here\nOpenRefine - Download here\nSample data - Download dirty_data.csv\n\n\n\nTechnical requirements:\n\nStable internet connection\nWorking camera and microphone (for interaction)\nA folder with some of your research files (for Archifiltre exploration)\n\n\n\n\nIf you encounter any installation issues or have questions, please contact:\nJulien Rabaud\njulien.rabaud@univ-pau.fr",
    "crumbs": [
      "Workshop",
      "Home"
    ]
  },
  {
    "objectID": "index.html#workshop-schedule",
    "href": "index.html#workshop-schedule",
    "title": "Personal and Research Data Management Organize, Clean & Enrich",
    "section": "Workshop Schedule",
    "text": "Workshop Schedule\n\n\n\nTime\nActivity\n\n\n\n\n0:00 - 0:10\nIntroduction & FAIR Principles\n\n\n0:10 - 0:30\nArchifiltre Demo & Hands-on\n\n\n0:30 - 0:35\nBreak\n\n\n0:35 - 0:55\nStorage & Backup Best Practices\n\n\n0:55 - 1:55\nOpenRefine: Data Cleaning Workshop\n\n\n1:55 - 2:00\nQ&A & Wrap-up",
    "crumbs": [
      "Workshop",
      "Home"
    ]
  },
  {
    "objectID": "index.html#additional-support",
    "href": "index.html#additional-support",
    "title": "Personal and Research Data Management Organize, Clean & Enrich",
    "section": "Additional Support",
    "text": "Additional Support\nAfter the workshop, you can:\n\nReview all materials on this website\nConsult the additional resources pages\nReach out for individual support\n\n\n\n\n\n\n\nNoteCC0 License\n\n\n\nAll workshop materials are released under a CC0 1.0 Universal license. You are free to use, modify, and share them.",
    "crumbs": [
      "Workshop",
      "Home"
    ]
  },
  {
    "objectID": "resources/archifiltre.html",
    "href": "resources/archifiltre.html",
    "title": "File Organization Tools",
    "section": "",
    "text": "Archifiltre is our recommended tool for visualizing and organizing your file structure. It is a free, open-source tool developed by French archivists, particularly relevant for humanities researchers.\n\n\n\nVisual treemap of your entire directory structure\nDuplicate detection across folders\nDate-based filtering to identify old/unused files\nTagging system for organizing files\nExport reports for documentation\n\n\n\n\n\nVisualize research archives - see your entire project structure at a glance\nIdentify duplicates - find redundant files across versions\nDate analysis - track when files were created/modified across long-term projects\nPrepare for data deposit - organize before archiving in institutional repositories\nDocument collections - generate reports for data management plans\n\n\n\n\n\n\nManaging sources:\n\nOrganize interview recordings\nTrack multiple transcription versions\nMap archival photograph collections\nIdentify gaps in data collection\n\n\nProject documentation:\n\nPrepare data for deposit (Zenodo, Nakala)\nDocument for Data Management Plans\nClean up before thesis submission\nCreate inventory for collaborators\n\n\n\n\n\n\n\nOfficial website: https://archifiltre.fabrique.social.gouv.fr/\nDownload: Available for Windows, macOS, and Linux\nDocumentation: User guide (mostly in French)\nSource code: GitHub repository\n\n\n\n\nWhile most Archifiltre resources are in French, the visual interface is intuitive:\n\nArchifiltre Demo Video (FR with English subtitles)",
    "crumbs": [
      "Additional Resources",
      "File Organization Tools"
    ]
  },
  {
    "objectID": "resources/archifiltre.html#archifiltre",
    "href": "resources/archifiltre.html#archifiltre",
    "title": "File Organization Tools",
    "section": "",
    "text": "Archifiltre is our recommended tool for visualizing and organizing your file structure. It is a free, open-source tool developed by French archivists, particularly relevant for humanities researchers.\n\n\n\nVisual treemap of your entire directory structure\nDuplicate detection across folders\nDate-based filtering to identify old/unused files\nTagging system for organizing files\nExport reports for documentation\n\n\n\n\n\nVisualize research archives - see your entire project structure at a glance\nIdentify duplicates - find redundant files across versions\nDate analysis - track when files were created/modified across long-term projects\nPrepare for data deposit - organize before archiving in institutional repositories\nDocument collections - generate reports for data management plans\n\n\n\n\n\n\nManaging sources:\n\nOrganize interview recordings\nTrack multiple transcription versions\nMap archival photograph collections\nIdentify gaps in data collection\n\n\nProject documentation:\n\nPrepare data for deposit (Zenodo, Nakala)\nDocument for Data Management Plans\nClean up before thesis submission\nCreate inventory for collaborators\n\n\n\n\n\n\n\nOfficial website: https://archifiltre.fabrique.social.gouv.fr/\nDownload: Available for Windows, macOS, and Linux\nDocumentation: User guide (mostly in French)\nSource code: GitHub repository\n\n\n\n\nWhile most Archifiltre resources are in French, the visual interface is intuitive:\n\nArchifiltre Demo Video (FR with English subtitles)",
    "crumbs": [
      "Additional Resources",
      "File Organization Tools"
    ]
  },
  {
    "objectID": "resources/archifiltre.html#alternative-tools-for-file-organization",
    "href": "resources/archifiltre.html#alternative-tools-for-file-organization",
    "title": "File Organization Tools",
    "section": "Alternative Tools for File Organization",
    "text": "Alternative Tools for File Organization\nIf you’re looking for more widely-used international tools, here are excellent alternatives:\n\n1. WinDirStat (Windows) / Disk Inventory X (macOS)\nPurpose: Visualize disk space usage\n\n\nFeatures: - Treemap visualization of disk usage - Identify large files taking up space - File type distribution analysis - Completely free and open-source\nBest for: Finding what’s eating up your storage\n\n\n\n\nWinDirStat visualization\n\n\n\n\nDownload:\n\nWinDirStat (Windows): https://windirstat.net/\nDisk Inventory X (macOS): http://www.derlien.com/\nBaobab (Linux): Pre-installed on most distributions\n\n\n\n\n2. WinMerge (Windows) / Meld (Cross-platform)\nPurpose: Compare and synchronize folders\nFeatures:\n\nSide-by-side folder comparison\nHighlight differences between directory structures\nIdentify duplicate, missing, or modified files\nMerge or sync folders selectively\nGreat for backup verification\n\nUse cases:\n\nCompare your working folder with your backup\nFind files that changed since last backup\nIdentify duplicates across different locations\n\nDownload:\n\nWinMerge (Windows): https://winmerge.org/\nMeld (Windows/macOS/Linux): https://meldmerge.org/\n\nDocumentation:\n\nWinMerge User Manual\nMeld User Guide\n\n\n\n\n3. FreeFileSync\nPurpose: Backup and file synchronization\nFeatures:\n\nMirror, update, or two-way sync between folders\nVisual comparison of source and target\nFilter rules (include/exclude patterns)\nBatch jobs for automated backups\nWorks with local drives, network shares, and cloud storage\n\nPerfect for:\n\nAutomated backup routines\nKeeping multiple copies in sync\nImplementing the 3-2-1 backup rule\n\nDownload: https://freefilesync.org/\nTutorial:\n\nFreeFileSync Beginner’s Guide\nVideo Tutorial\n\n\n\n\n4. TreeSize Free (Windows)\nPurpose: Detailed disk space analysis\nFeatures:\n\nShow folder sizes in Windows Explorer\nDrill down to find large files\nExport results to Excel\nScheduled scans\n\nDownload: https://www.jam-software.com/treesize_free",
    "crumbs": [
      "Additional Resources",
      "File Organization Tools"
    ]
  },
  {
    "objectID": "resources/archifiltre.html#comparison-table",
    "href": "resources/archifiltre.html#comparison-table",
    "title": "File Organization Tools",
    "section": "Comparison Table",
    "text": "Comparison Table\n\n\n\n\n\n\n\n\n\nTool\nPlatform\nBest For\nFree/Open Source\n\n\n\n\nArchifiltre\nWin/Mac/Linux\nOverall file organization & documentation\n✅ Yes\n\n\nWinDirStat\nWindows\nDisk space visualization\n✅ Yes\n\n\nDisk Inventory X\nmacOS\nDisk space visualization\n✅ Yes\n\n\nWinMerge\nWindows\nFolder comparison & sync\n✅ Yes\n\n\nMeld\nWin/Mac/Linux\nFolder comparison & sync\n✅ Yes\n\n\nFreeFileSync\nWin/Mac/Linux\nBackup & synchronization\n✅ Yes\n\n\nTreeSize Free\nWindows\nDisk space analysis\n✅ Free version",
    "crumbs": [
      "Additional Resources",
      "File Organization Tools"
    ]
  },
  {
    "objectID": "resources/archifiltre.html#recommended-workflow",
    "href": "resources/archifiltre.html#recommended-workflow",
    "title": "File Organization Tools",
    "section": "Recommended Workflow",
    "text": "Recommended Workflow\nFor comprehensive file organization, consider using multiple tools:\n\nVisualization: Start with Archifiltre or WinDirStat to see what you have\nCleanup: Use dupeGuru to find and remove duplicates\nOrganization: Reorganize based on insights\nBackup: Use FreeFileSync for automated backups\nVerification: Use WinMerge to verify backup integrity",
    "crumbs": [
      "Additional Resources",
      "File Organization Tools"
    ]
  },
  {
    "objectID": "resources/archifiltre.html#tips-for-choosing-tools",
    "href": "resources/archifiltre.html#tips-for-choosing-tools",
    "title": "File Organization Tools",
    "section": "Tips for Choosing Tools",
    "text": "Tips for Choosing Tools\n\n\n\n\n\n\nTipSelection Criteria\n\n\n\n\nStart simple: If you’re overwhelmed, begin with just one tool (Archifiltre is a good all-rounder)\nMatch to your OS: Some tools work better on specific platforms\nConsider automation: If you need regular backups, FreeFileSync is ideal\nOpen source preference: All tools listed here have free/open-source options",
    "crumbs": [
      "Additional Resources",
      "File Organization Tools"
    ]
  },
  {
    "objectID": "resources/archifiltre.html#further-reading---digital-humanities",
    "href": "resources/archifiltre.html#further-reading---digital-humanities",
    "title": "File Organization Tools",
    "section": "Further Reading - Digital Humanities",
    "text": "Further Reading - Digital Humanities\n\nData Management for DH\n\nDARIAH Guide to Data Management - European infrastructure for arts & humanities\nCollections as Data - Making collections computational\nDHQ: Digital Humanities Quarterly - Methodology articles\n\n\n\nEuropean Infrastructure\n\nEuropeana Research - European cultural heritage\nHuma-Num Services - French SSH infrastructure\nDARIAH Working Groups - Thematic networks\n\n\n\nProject Examples\n\nVenice Time Machine - Historical digitization\nPelagios Network - Linking places in history\nMapping the Republic of Letters - Correspondence networks\n\n\n\n\n\n\n\n\nNoteQuestions?\n\n\n\nIf you need help choosing the right tool for your specific needs, feel free to contact me: julien.rabaud@univ-pau.fr",
    "crumbs": [
      "Additional Resources",
      "File Organization Tools"
    ]
  },
  {
    "objectID": "resources/openrefine.html",
    "href": "resources/openrefine.html",
    "title": "OpenRefine for Digital Humanities",
    "section": "",
    "text": "OpenRefine is an essential tool for Digital Humanities researchers working with messy, heterogeneous data from diverse sourcesâ€”archives, surveys, databases, and digitized collections.\n\n\n\n\n\n\nNoteWhy OpenRefine for SSH Research?\n\n\n\n\nClean inconsistent names (people, places, institutions)\nStandardize dates across different systems\nReconcile data with authority files (VIAF, GeoNames, Wikidata)\nPrepare data for visualization and analysis\nReproducible - export cleaning scripts to reuse",
    "crumbs": [
      "Additional Resources",
      "OpenRefine Tutorials"
    ]
  },
  {
    "objectID": "resources/openrefine.html#introduction-to-openrefine",
    "href": "resources/openrefine.html#introduction-to-openrefine",
    "title": "OpenRefine for Digital Humanities",
    "section": "",
    "text": "OpenRefine is an essential tool for Digital Humanities researchers working with messy, heterogeneous data from diverse sourcesâ€”archives, surveys, databases, and digitized collections.\n\n\n\n\n\n\nNoteWhy OpenRefine for SSH Research?\n\n\n\n\nClean inconsistent names (people, places, institutions)\nStandardize dates across different systems\nReconcile data with authority files (VIAF, GeoNames, Wikidata)\nPrepare data for visualization and analysis\nReproducible - export cleaning scripts to reuse",
    "crumbs": [
      "Additional Resources",
      "OpenRefine Tutorials"
    ]
  },
  {
    "objectID": "resources/openrefine.html#getting-started",
    "href": "resources/openrefine.html#getting-started",
    "title": "OpenRefine for Digital Humanities",
    "section": "Getting Started",
    "text": "Getting Started\n\nOfficial Documentation\n\nUser Manual: https://openrefine.org/docs\n\nComplete guide to all features\nGREL (expression language) reference\nFAQ and troubleshooting\n\nCommunity Forum: https://forum.openrefine.org/\n\nActive community\nSearch before posting - many answers already exist",
    "crumbs": [
      "Additional Resources",
      "OpenRefine Tutorials"
    ]
  },
  {
    "objectID": "resources/openrefine.html#video-tutorials---by-level",
    "href": "resources/openrefine.html#video-tutorials---by-level",
    "title": "OpenRefine for Digital Humanities",
    "section": "Video Tutorials - By Level",
    "text": "Video Tutorials - By Level\n\n\n\n\n\n\nNoteNote on Video Resources\n\n\n\nVideo tutorials can become outdated as OpenRefine evolves. Always check:\n\nThe official OpenRefine documentation: https://openrefine.org/docs\nThe external resources page: https://openrefine.org/external_resources\nThe OpenRefine forum: https://forum.openrefine.org/\n\nFor the most current video tutorials and recommendations.\n\n\n\nBeginner Level (Start Here!)\n\n1. Library Carpentry OpenRefine\nBest comprehensive introduction for humanities scholars\nInteractive written lesson:\n\nURL: https://librarycarpentry.github.io/lc-open-refine/\nSelf-paced: 3-4 hours\nIncludes: Practice datasets\nWhy it’s great: Designed for librarians and humanities researchers\nUpdated regularly - tested with OpenRefine 3.7.7\n\nContent covered:\n\nInstalling OpenRefine\nCreating projects\nFaceting and filtering\nBasic transformations\nClustering\nGREL expressions\n\n\n\n\n2. OpenRefine Tutorial for Digital Public Library of America\nRecent step-by-step video walkthrough\n\nVideo: OpenRefine Tutorial 2024\nDuration: ~50 minutes\nBest for: Visual learners who prefer watching demonstrations\nCovers: Basic operations, faceting, clustering, and data cleaning\nWhy it’s good: Recent, clear demonstrations with current OpenRefine interface\n\n\n\n\n3. Programming Historian - Cleaning Data\nText-based tutorial with cultural heritage examples\n\nTutorial: Cleaning Data with OpenRefine\nLevel: Beginner\nTime: 1-2 hours\nExample: Museum collection data (Powerhouse Museum)\nStrength: Screenshots for every step, clear explanations, peer-reviewed\nLast updated: March 2024\n\n\n\n\n4. OpenRefine Beginners Tutorial by Emma Carroll\nComprehensive video covering reconciliation and Wikidata\n\nVideo: Available on Media Hopper Create and Wikimedia Commons\nDuration: ~12 minutes\nCovers: Opening files, reconciling data, building schemas, uploading to Wikidata\nBest for: Those interested in linking data to authority files\n\n\n\n\n\nIntermediate Level\n\n4. Advanced Clustering & Reconciliation\nStanford University Workshop\n\nVideo: Stanford OpenRefine Workshop\nDuration: 90 minutes\nTopics:\n\nAdvanced clustering algorithms\nWorking across multiple columns\nData reconciliation\nExporting clean data\n\n\n\n\n\n5. University of Toronto - OpenRefine Tutorials\nComprehensive tutorial series with video support\n\nWebsite: U of T Map and Data Library OpenRefine\nVideo course: Available for U of T community (Quercus platform)\nContent: Multiple activities covering:\n\nSurvey data cleaning\nCitizen science data\nRegular expressions (Regex)\nUsing reconciliation services\nWorking with APIs\n\nWhy it’s good: Well-structured, progressive learning path\n\n\n\n\n6. GREL: Expression Language\nIntroduction to GREL transformations\n\nVideo: GREL Statements in OpenRefine\nDuration: ~20 minutes\nLevel: Intermediate\nTopics covered:\n\nGREL syntax basics\nString manipulation operations\nUsing conditional statements (if/else)\nWorking with dates\nCommon transformation patterns\n\nBest for: Understanding how to write custom transformations beyond basic menu operations\n\nComplementary text resources:\n\nOfficial GREL documentation: https://openrefine.org/docs/manual/grel\nGREL functions reference: https://openrefine.org/docs/manual/grelfunctions\nGREL recipes (GitHub wiki): Practical examples and common use cases\n\n\n\n\n\nAdvanced Level\n\n7. Reconciliation with Authority Files\nEssential for cultural heritage - match names to standard vocabularies\nWritten guide:\n\nReconciliation Services Overview\n\n\n\n\n8. API Calls & Web Scraping\nEnrich data from external sources\n\nBlog post: Tony Hirst - OpenRefine & APIs\nTopics:\n\nHTTP requests\nParsing JSON\nRate limiting\nError handling",
    "crumbs": [
      "Additional Resources",
      "OpenRefine Tutorials"
    ]
  },
  {
    "objectID": "resources/openrefine.html#text-based-tutorials---recommended",
    "href": "resources/openrefine.html#text-based-tutorials---recommended",
    "title": "OpenRefine for Digital Humanities",
    "section": "Text-Based Tutorials - Recommended",
    "text": "Text-Based Tutorials - Recommended\n\n1. Library Carpentry (Comprehensive)\nThe gold standard\n\nURL: https://librarycarpentry.org/lc-open-refine/\nTime: 3-4 hours self-paced\nFormat: Interactive web lesson with datasets\nCovers:\n\nImporting data\nInterface overview\nFaceting and filtering\nClustering\nTransforming data with GREL\nExporting and scripts\n\n\nWhy it’s excellent: No coding background needed, humanities-friendly\n\n\n\n2. Programming Historian Series\nMultiple OpenRefine lessons for different tasks\n\nCleaning Data\nFetching and Parsing Data from APIs\nWorking with Linked Open Data\n\nStrength: Step-by-step, peer-reviewed, maintained\n\n\n\n3. Data Carpentry: Social Science Data\nDiscipline-specific approach for survey/interview data\n\nURL: https://datacarpentry.org/openrefine-socialsci/\nTime: 2 hours\nFocus: Survey data, qualitative coding\nIncludes: Practice dataset\n\n\n\n\n4. GESIS: OpenRefine Tutorial\nAcademic-focused, social sciences\n\nURL: https://www.gesis.org/en/services/research/tools/openrefine\nFormat: PDF guide + datasets\nLevel: Beginner to Intermediate",
    "crumbs": [
      "Additional Resources",
      "OpenRefine Tutorials"
    ]
  },
  {
    "objectID": "resources/openrefine.html#openrefine-for-specific-dh-tasks",
    "href": "resources/openrefine.html#openrefine-for-specific-dh-tasks",
    "title": "OpenRefine for Digital Humanities",
    "section": "OpenRefine for Specific DH Tasks",
    "text": "OpenRefine for Specific DH Tasks\n\nCleaning Archival Metadata\nCommon issues in archival data:\n\nInconsistent date formats (12/3/1887 vs March 12, 1887)\nVariant spellings (François vs Francois)\nMultiple languages (Rome, Roma, RÃ´me)\nIncomplete records\n\nOpenRefine solutions:\n\nClustering to find variants\nGREL to standardize dates\nFacets to identify gaps\nReconciliation to match authority records\n\nExample workflow:\n1. Import metadata CSV\n2. Cluster names (Edit cells &gt; Cluster and edit)\n3. Standardize dates (Transform &gt; value.toDate())\n4. Fill down missing data (Edit cells &gt; Fill down)\n5. Reconcile with VIAF (Reconcile &gt; Start reconciling)\n\n\n\nWorking with Historical Names & Places\nChallenges:\n\nHistorical spellings (Wien vs Vienna, Pressburg vs Bratislava)\nName changes over time\nVernacular vs standardized forms\nGeoNames reconciliation - modern place names\nPleiades - ancient places (https://pleiades.stoa.org/)\nWikidata - multilingual, historical entities\nGetty TGN - Thesaurus of Geographic Names\n\nTutorial:\n\nReconciling Historical Place Names (includes OpenRefine)\n\n\n\n\nPreparing Data for Visualization\nFrom messy spreadsheet to clean dataset:\nBefore OpenRefine:\nDate       | Event              | Location\n3/12/1848  | Revolution         | paris\nMarch 1848 | Protests           | PARIS\n12.03.1848 | Declaration signed | Paris, France\nAfter OpenRefine:\nDate       | Event              | City  | Country\n1848-03-12 | Revolution         | Paris | France\n1848-03-01 | Protests           | Paris | France\n1848-03-12 | Declaration signed | Paris | France\nNow ready for: Timeline visualization, mapping, network analysis\n\n\n\nTranscription Quality Control\nUsing OpenRefine to validate crowdsourced transcriptions:\n\nImport transcriptions from Transkribus, FromThePage, etc.\nCluster variant spellings\nFacet by transcriber to check consistency\nFilter blank/incomplete entries\nFlag suspicious values for review",
    "crumbs": [
      "Additional Resources",
      "OpenRefine Tutorials"
    ]
  },
  {
    "objectID": "resources/openrefine.html#grel-google-refine-expression-language",
    "href": "resources/openrefine.html#grel-google-refine-expression-language",
    "title": "OpenRefine for Digital Humanities",
    "section": "GREL (Google Refine Expression Language)",
    "text": "GREL (Google Refine Expression Language)\n\nEssential GREL Functions\n\nBasic String Operations\n# Remove whitespace\nvalue.trim()\n\n# Change case\nvalue.toLowercase()\nvalue.toUppercase()\nvalue.toTitlecase()\n\n# Replace text\nvalue.replace(\"old\", \"new\")\nvalue.replace(/[^a-zA-Z]/, \"\")    # Regex: keep only letters\n\n# Extract parts\nvalue.substring(0, 4)              # First 4 characters\nvalue.split(\",\")[0]                # Split by comma, take first\n\n\nWorking with Dates\n# Parse date strings\nvalue.toDate()\nvalue.toDate(\"dd/MM/yyyy\")         # Specify format\n\n# Format dates\nvalue.toDate().toString(\"yyyy-MM-dd\")\nvalue.toDate().toString(\"MMMM dd, yyyy\")\n\n# Date components\nvalue.toDate().year()\nvalue.toDate().month()\n\n\nConditional Logic\n# If-then-else\nif(value == \"N/A\", null, value)\nif(value.length() &gt; 10, \"long\", \"short\")\n\n# Multiple conditions\nif(value.contains(\"manuscript\"), \n   \"MS\", \n   if(value.contains(\"book\"), \"Book\", \"Other\"))\n\n\nArray Operations\n# Split and join\nvalue.split(\";\")                   # Create array\nvalue.split(\";\").join(\" | \")       # Split then rejoin\nforEach(value.split(\",\"), v, v.trim())  # Trim each element\n\n\nCross-Column References\n# Reference another column\ncells[\"Column Name\"].value\n\n# Combine columns\ncells[\"First\"].value + \" \" + cells[\"Last\"].value\n\n# Conditional based on other column\nif(cells[\"Language\"].value == \"Latin\", \"LA\", \"EN\")\n\n\n\n\nGREL Recipes for Cultural Heritage\n\n1. Standardize Multiple Date Formats\n# Handle: \"12/3/1887\", \"March 12 1887\", \"1887-03-12\"\nif(value.contains(\"/\"), \n   value.toDate(\"dd/MM/yyyy\"), \n   if(value.contains(\"-\"),\n      value.toDate(\"yyyy-MM-dd\"),\n      value.toDate(\"MMMM dd yyyy\")\n   )\n).toString(\"yyyy-MM-dd\")\n\n\n2. Clean Shelfmark/Call Numbers\n# \"Ms. Lat. 123\" → \"MS-LAT-123\"\nvalue.toUppercase()\n     .replace(/[^A-Z0-9]+/, \"-\")\n     .replace(/^-|-$/, \"\")\n\n\n3. Extract Folio/Page Numbers\n# From \"fol. 23r\" or \"f.23v\" → \"23r\"\nvalue.replace(/^f(ol)?\\.?\\s*/i, \"\")\n\n\n4. Transliterate Characters\n# Basic Latin to ASCII (remove diacritics)\nvalue.replace(\"é\", \"e\")\n     .replace(\"à\", \"a\")\n     .replace(\"ü\", \"u\")\n     # etc.\n\n\n5. Generate URLs from Identifiers\n# Create archive.org link from ID\n\"https://archive.org/details/\" + value\n\n# BnF Gallica link\n\"https://gallica.bnf.fr/ark:/12148/\" + value\n\n\n\n\nGREL Cheat Sheet\nOfficial reference: https://openrefine.org/docs/manual/grel\nPrintable cheat sheet:\n\nGREL Quick Reference (GitHub Gist)",
    "crumbs": [
      "Additional Resources",
      "OpenRefine Tutorials"
    ]
  },
  {
    "objectID": "resources/openrefine.html#reconciliation-services-for-dh",
    "href": "resources/openrefine.html#reconciliation-services-for-dh",
    "title": "OpenRefine for Digital Humanities",
    "section": "Reconciliation Services for DH",
    "text": "Reconciliation Services for DH\n\nWhat is Reconciliation?\nMatch your data against authoritative sources:\n\nStandardize names → VIAF, ISNI\nPlaces → GeoNames, Wikidata\nConcepts → Getty AAT, LCSH\nHistorical entities → Wikidata, DBpedia\n\n\n\nBuilt-in Services\n\nWikidata - Universal multilingual knowledge base\nVIAF - Virtual International Authority File (people, institutions)\nGeoNames - Geographic names database\n\n\n\nAdding Custom Services\nFor Cultural Heritage:\n\nGetty Vocabularies (AAT, ULAN, TGN)\n\nGuide: https://www.getty.edu/research/tools/vocabularies/\n\nFAST (Faceted Application of Subject Terminology)\n\nhttps://www.oclc.org/research/areas/data-science/fast.html\n\nPleiades (Ancient places)\n\nhttps://pleiades.stoa.org/\n\n\nHow to add:\n\nReconcile &gt; Start reconciling\nAdd Standard Service\nEnter reconciliation endpoint URL\n\nTutorial: Using Reconciliation Services",
    "crumbs": [
      "Additional Resources",
      "OpenRefine Tutorials"
    ]
  },
  {
    "objectID": "resources/openrefine.html#practice-datasets-for-dh",
    "href": "resources/openrefine.html#practice-datasets-for-dh",
    "title": "OpenRefine for Digital Humanities",
    "section": "Practice Datasets for DH",
    "text": "Practice Datasets for DH\n\n1. Museum/Gallery Collections\n\nCarnegie Museum of Art: https://github.com/cmoa/collection\n\n30,000+ artworks with metadata\nCSV format, ready to clean\n\nTate Collection: https://github.com/tategallery/collection\n\n70,000+ artworks and artists\nVarious data quality issues\n\nRijksmuseum: https://data.rijksmuseum.nl/\n\nDutch masters collection\nMessy creator names, dates\n\n\n\n\n\n2. Historical Datasets\n\n19th Century British Library Books:\n\nhttps://doi.org/10.23636/1112\nOCR text, cataloging data\n\nColonial Correspondence:\n\nUnsilencing Colonial Archives\nNamed entity recognition dataset\n\nHistorical Census Data:\n\nVarious historical census projects\nInconsistent place names, occupations\n\n\n\n\n\n3. Archival Finding Aids\n\nEAD (Encoded Archival Description) exports\n\nConvert to CSV/Excel\nClean dates, names, locations\n\n\nExample workflow:\n\nExport EAD as spreadsheet\nImport to OpenRefine\nStandardize date ranges\nReconcile place names\nExport clean version\n\n\n\n\n4. OpenRefine Sample Datasets\n\nOfficial examples: https://github.com/OpenRefine/OpenRefine/wiki/Sample-Datasets",
    "crumbs": [
      "Additional Resources",
      "OpenRefine Tutorials"
    ]
  },
  {
    "objectID": "resources/openrefine.html#case-studies-openrefine-in-dh-projects",
    "href": "resources/openrefine.html#case-studies-openrefine-in-dh-projects",
    "title": "OpenRefine for Digital Humanities",
    "section": "Case Studies: OpenRefine in DH Projects",
    "text": "Case Studies: OpenRefine in DH Projects\n\n1. Mapping the Republic of Letters\nProject: Stanford visualization of early modern correspondence\nOpenRefine used for:\n\nCleaning 55,000+ letters\nStandardizing correspondent names\nGeocoding locations\nReconciling with VIAF\n\nResult: Network visualization, interactive map\nMore info: http://republicofletters.stanford.edu/\n\n\n\n2. Pelagios Project\nProject: Linking historical places across ancient sources\nOpenRefine workflow:\n\nImport various gazetteers\nStandardize place name variants\nReconcile with Pleiades\nGenerate RDF/Linked Data\n\nMore info: https://pelagios.org/\n\n\n\n3. British Library Labs\nVarious projects using BL datasets\nCommon OpenRefine tasks:\n\nOCR error correction\nMetadata enhancement\nClassification/tagging\nPreparing for visualization\n\nCompetitions: https://blogs.bl.uk/digital-scholarship/",
    "crumbs": [
      "Additional Resources",
      "OpenRefine Tutorials"
    ]
  },
  {
    "objectID": "resources/openrefine.html#openrefine-extensions",
    "href": "resources/openrefine.html#openrefine-extensions",
    "title": "OpenRefine for Digital Humanities",
    "section": "OpenRefine Extensions",
    "text": "OpenRefine Extensions\n\nRecommended Extensions for DH\n\n1. RDF Extension\nFor Linked Open Data projects\n\nInfo: https://openrefine.org/docs/manual/rdfextension\nUse: Generate RDF triples, work with ontologies\nExample: Creating LOD from museum collections\n\n\n\n2. Named Entity Recognition (NER)\nExtract entities from text automatically\n\nExtension: https://github.com/stkenny/Refine-NER-Extension\nUse: Identify people, places, organizations in corpus\nExample: Processing historical documents\n\n\n\n3. Wikibase Extension\nFor projects using Wikibase (like Wikidata)\n\nInfo: https://openrefine.org/docs/manual/wikibase\nUse: Upload data to institutional Wikibase\nExample: Creating structured cultural heritage database",
    "crumbs": [
      "Additional Resources",
      "OpenRefine Tutorials"
    ]
  },
  {
    "objectID": "resources/openrefine.html#performance-tips",
    "href": "resources/openrefine.html#performance-tips",
    "title": "OpenRefine for Digital Humanities",
    "section": "Performance Tips",
    "text": "Performance Tips\nFor large cultural heritage datasets:\n\nFilter first - narrow data before complex operations\nClose unused facets - they consume memory\nExport regularly - don’t rely only on auto-save\nUse batches - process 10k rows at a time\nDocument workflow - export operations as JSON",
    "crumbs": [
      "Additional Resources",
      "OpenRefine Tutorials"
    ]
  },
  {
    "objectID": "resources/openrefine.html#going-further---advanced-topics",
    "href": "resources/openrefine.html#going-further---advanced-topics",
    "title": "OpenRefine for Digital Humanities",
    "section": "Going Further - Advanced Topics",
    "text": "Going Further - Advanced Topics\n\n1. Regular Expressions (Regex)\nEssential for pattern matching in historical text\nLearn regex:\n\nInteractive tutorial: https://regexone.com/\nTester: https://regex101.com/\nFor historians: Regex for Archival Research\n\nExample regex for OpenRefine:\n# Match folio numbers (f. 23r, fol. 45v, etc.)\nvalue.match(/f(ol)?\\.?\\s*(\\d+[rv]?)/)\n\n# Extract years (1500-1999)\nvalue.match(/\\b(1[5-9]\\d{2})\\b/)\n\n# Find capitalized words (potential names)\nvalue.match(/\\b[A-Z][a-z]+\\b/)\n\n\n\n2. Batch Processing Multiple Files\nProcess many similar files with same operations:\n\nCreate and test workflow on sample file\nExport operations (Extract… in Undo/Redo tab)\nApply to new files (Apply… with saved JSON)\n\nUse case: Cleaning multiple finding aids, multiple years of data\n\n\n\n3. Combining OpenRefine with Other Tools\nTypical DH workflow:\n1. Data collection (archives, databases)\n2. Digitization/transcription\n3. Initial organization (Tropy, file system)\n4. ↓\n5. OPENREFINE: Clean, standardize, reconcile\n6. ↓\n7. Export clean data\n8. Analysis (R, Python, Gephi, Palladio)\n9. Visualization (maps, networks, timelines)\n10. Publication (online exhibit, article, data paper)",
    "crumbs": [
      "Additional Resources",
      "OpenRefine Tutorials"
    ]
  },
  {
    "objectID": "resources/openrefine.html#complementary-tools",
    "href": "resources/openrefine.html#complementary-tools",
    "title": "OpenRefine for Digital Humanities",
    "section": "Complementary Tools",
    "text": "Complementary Tools\n\nBefore OpenRefine\n\nTropy - organize images, extract metadata\nTranskribus - transcribe handwritten sources\ncsvkit - command-line CSV manipulation (for very large files)\n\n\n\nAfter OpenRefine\nFor analysis & visualization:\n\nPalladio - Network & geo visualization (Stanford)\n\nhttps://hdlab.stanford.edu/palladio/\n\nGephi - Network analysis\n\nhttps://gephi.org/\n\nVoyant Tools - Text analysis\n\nhttps://voyant-tools.org/\n\nR / Python - Statistical analysis, custom visualizations\n\nR: tidyverse package\nPython: pandas library\n\n\n\n\nIntegrated workflows\n\nZotero → metadata export → OpenRefine → clean data\nOmeka → item export → OpenRefine → enhanced metadata → Omeka import",
    "crumbs": [
      "Additional Resources",
      "OpenRefine Tutorials"
    ]
  },
  {
    "objectID": "resources/openrefine.html#learning-resources---digital-humanities-focus",
    "href": "resources/openrefine.html#learning-resources---digital-humanities-focus",
    "title": "OpenRefine for Digital Humanities",
    "section": "Learning Resources - Digital Humanities Focus",
    "text": "Learning Resources - Digital Humanities Focus\n\nOnline Courses\n\nDARIAH-Campus\n\nhttps://campus.dariah.eu/\nEuropean DH training platform\nIncludes data management modules\n\nProgramming Historian\n\nhttps://programminghistorian.org/\nPeer-reviewed tutorials\nOpenRefine + other DH tools\n\nPARTHENOS Training\n\nhttp://training.parthenos-project.eu/\nResearch infrastructure training\nCultural heritage specific\n\n\n\n\n\nBooks & Long-Form Guides\n\n“Using OpenRefine” - Packt Publishing\n\nRuben Verborgh, Max De Wilde\nComprehensive, includes real projects\n\n“The Historian’s Macroscope”\n\nhttp://www.themacroscope.org/\nIncludes OpenRefine chapter for historians\n\n“The Programming Historian” - Lessons\n\nhttps://programminghistorian.org/en/lessons/\nMultiple OpenRefine tutorials\n\n\n\n\n\nCommunities\nWhere to ask questions:\n\nOpenRefine Forum\n\nhttps://forum.openrefine.org/\nVery active, friendly community\nSearch before asking!\n\nDH Slack\n\nhttps://digitalhumanities.slack.com/\n#openrefine channel\nReal-time help\n\nDigital Humanities Questions (DHQ)\n\nhttps://digitalhumanities.stackexchange.com/\nQ&A format",
    "crumbs": [
      "Additional Resources",
      "OpenRefine Tutorials"
    ]
  },
  {
    "objectID": "resources/openrefine.html#resources-in-other-languages",
    "href": "resources/openrefine.html#resources-in-other-languages",
    "title": "OpenRefine for Digital Humanities",
    "section": "Resources in Other Languages",
    "text": "Resources in Other Languages\n\nFrench Resources\nDoRANum materials (some in French): - https://doranum.fr/ - Video guides, fact sheets - Check for English subtitles\nHuma-Num training: - https://www.huma-num.fr/formations - Sometimes offers OpenRefine workshops\nHuma-Num tools: - https://www.huma-num.fr/services-et-outils\n\n\nOther European Languages\n\nItalian: AIuCD resources\nGerman: DHd tutorials\nSpanish: La Red de Humanidades Digitales",
    "crumbs": [
      "Additional Resources",
      "OpenRefine Tutorials"
    ]
  },
  {
    "objectID": "resources/openrefine.html#final-tips-for-ssh-researchers",
    "href": "resources/openrefine.html#final-tips-for-ssh-researchers",
    "title": "OpenRefine for Digital Humanities",
    "section": "Final Tips for SSH Researchers",
    "text": "Final Tips for SSH Researchers\n\n\n\n\n\n\nTipBest Practices\n\n\n\nFor long-term projects:\n\nDocument everything - Export operations as JSON, keep notes\nVersion your datasets - Name files with dates and versions\nTest workflows early - Don’t wait until deadline to clean data\nShare knowledge - Help colleagues learn OpenRefine\nThink FAIR - Clean data is the first step to FAIR data\n\nRemember:\n\nOpenRefine is non-destructive - you can always undo\nOperations are reproducible - save and reuse workflows\nThe community is helpful - don’t hesitate to ask questions",
    "crumbs": [
      "Additional Resources",
      "OpenRefine Tutorials"
    ]
  },
  {
    "objectID": "resources/openrefine.html#questions-support",
    "href": "resources/openrefine.html#questions-support",
    "title": "OpenRefine for Digital Humanities",
    "section": "Questions & Support",
    "text": "Questions & Support\nFor OpenRefine help:\n\nForum: https://forum.openrefine.org/\nDocumentation: https://openrefine.org/docs\n\nFor workshop-specific questions: julien.rabaud@univ-pau.fr",
    "crumbs": [
      "Additional Resources",
      "OpenRefine Tutorials"
    ]
  }
]